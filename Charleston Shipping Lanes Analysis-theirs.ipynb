{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charleston Shipping Lanes Risk Analysis\n",
    "\n",
    "### Research Question\n",
    "\n",
    "> Presently, there are no recommended lanes off Charleston. Would recommended lane/s reduce the relative risk of interactions between commercial vessels and right whales? Can it be determined?  If so, what is the expected reduction in risk of interactions and where should the lanes be placed? \n",
    "\n",
    "### Modeling Approach\n",
    "\n",
    "Following Fonnesbeck *et al.* (2008), we will construct a predictive model of right whale encounter that is based on existing aerial survey data and relevant environmental predictors of whale habitat use, such as bathymetry and water temperature. This model will be used as a tool to identify a shipping lane designation that minimizes risk to the migratory northern right whale population.\n",
    "\n",
    "The metric for risk for a particular lane designation will be the expected encounter rate of whale groups over the total area defined by any candidate lane. To avoid having to account for small-scale factors related to the interaction of whales and ships, risk will be estimated at a relatively coarse scale. Specifically, I will estimate the expected occurrence probability of whale groups over the cells of a 3 x 3 nmi grid in the southern part of the study area, and a 4 x 4 nmi grid in the north.\n",
    "\n",
    "A key component of the modeling approach is the estimating the rate of encounter with whale groups in each grid cell. These estimates are informed by aerial survey data, which includes both sightings and on-watch effort. The key parameter of interest is the encounter rate, which we will use to model the survey encounters as a Poisson random variable, using the survey effort as an offset.\n",
    "\n",
    "Because we are interested in identifying static lanes that do not change month-to-month, we will ignore the temporal dynamics of whale occurrence over the calving season, and pool the data regardless of month or year. This will greatly simplify the modeling and provide estimates that are based on overall whale habitat use.\n",
    "\n",
    "To aid the predictive performance of the model, we will use covariates to estimate encounter rates; specifically, we will use the distance to the 22C isotherm and mean grid cell depth as non-parametric regression terms to predict whale encounters. This will allow for arbitrary non-linear relationships between either covariate and whale habitat use.\n",
    "\n",
    "To account for spatial autocorrelation, we included a conditional auto-covariance function for the cells of the habitat grid, allowing information from adjacent grid cells to help inform one another, thereby improving model estimates. Observations among nearby cells are likely not independent, with whales sighted in one cell being very likely to visit adjacent cells within any 2-week period characterized by the aggregated survey data.\n",
    "\n",
    "External constraints on the identification of an optimal line may include factors such as a maximum distance to port and a minimum lane width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named basemap",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c62209b523cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchained_assignment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPatchCollection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named basemap"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.basemap import pyproj\n",
    "from matplotlib.collections import PatchCollection\n",
    "from pyproj import Proj, transform\n",
    "from datetime import date\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon, shape\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import urllib\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ship transits to GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transits = gpd.GeoDataFrame.from_file('data/transits.shp').replace({-999: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and project habitat model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HabModelEnviro = gpd.GeoDataFrame.from_file('data/HabModelEnviro.shp').replace({-999: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HabModelEnviroWGS84 = HabModelEnviro.to_crs({'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate to area of interest (31o41' to 33o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper_limit = 33\n",
    "lower_limit = 31.683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HabModelEnviroSC = HabModelEnviroWGS84[(lower_limit < HabModelEnviroWGS84.Lat) & \n",
    "                                       (HabModelEnviroWGS84.Lat < upper_limit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nav channel layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "navchannel = gpd.GeoDataFrame.from_file('data/SHEP_navchannel.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wgs84_crs = {'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'}\n",
    "navchannelWGS84 = navchannel.to_crs(wgs84_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import reaches data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reaches = gpd.GeoDataFrame.from_file('data/Charleston_Reaches_Post_45_Measures.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reachesWGS84 = reaches.to_crs(wgs84_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sma = gpd.GeoDataFrame.from_file(\"data/right_whale_sma_all/right_whale_SMA_all_po.shp\").to_crs(wgs84_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up base map of SC region for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basemap_sc = Basemap(ax=None,\n",
    "    llcrnrlon=-81.5,\n",
    "    llcrnrlat=31.6,\n",
    "    urcrnrlon=-77.5,\n",
    "    urcrnrlat=33.2,\n",
    "    resolution='i',\n",
    "    epsg='4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reaches locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = plt.gca()\n",
    "basemap_sc.drawstates(linewidth=0.15)\n",
    "basemap_sc.drawcoastlines(linewidth=0.25)\n",
    "basemap_sc.fillcontinents(zorder=-1, color='green', alpha=0.5)\n",
    "reachesWGS84.plot(axes=ax)\n",
    "navchannelWGS84.plot(axes=ax)\n",
    "sma.plot(axes=ax, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import survey data and reproject to WGS84, then clip to area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HabModelSurvey = gpd.GeoDataFrame.from_file('data/HabModelSurvey.shp')\n",
    "HabModelSurveyWGS84 = HabModelSurvey.to_crs({'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'})\n",
    "HabModelSurveySC = HabModelSurveyWGS84[(lower_limit < HabModelSurveyWGS84.Lat) \n",
    "                                       & (HabModelSurveyWGS84.Lat < upper_limit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a merge of survey and habitat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use lower case for trailing A and B times\n",
    "replace_dict = {c:c[:-1]+c[-1].lower() for c in HabModelEnviroSC.columns if c.endswith('A') or c.endswith('B')}\n",
    "HabModelEnviro_points = HabModelEnviroSC.copy().rename(columns=replace_dict)\n",
    "HabModelEnviro_points.geometry = HabModelEnviro_points.geometry.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replace_dict = {c:c[:-1]+c[-1].lower() for c in HabModelSurveySC.columns if c.endswith('A') or c.endswith('B')}\n",
    "HabModel_merged = sjoin(HabModelSurveySC.rename(columns=replace_dict), HabModelEnviro_points, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop cells with null DistToShor values (land)\n",
    "HabModel_merged = HabModel_merged.dropna(subset=['DistToShor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer distance to shore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HabModel_merged = HabModel_merged[HabModel_merged.DistToShor>1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data into long format and remove time information from column names. We are breaking the data into one table for each 2-week period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_intervals = ['Dec03a','Dec03b','Dec04a','Dec04b','Dec05a','Dec05b','Dec06a','Dec06b','Dec07a','Dec07b',\n",
    "         'Dec08a','Dec08b','Dec09a','Dec09b','Dec10a','Dec10b','Dec11a','Dec11b','Dec12a','Dec12b',\n",
    "         'Feb04a','Feb04b','Feb05a','Feb05b','Feb06a','Feb06b','Feb07a','Feb07b','Feb08a','Feb08b',\n",
    "         'Feb09a','Feb09b','Feb10a','Feb10b','Feb11a','Feb11b','Feb12a','Feb12b','Feb13a','Feb13b',\n",
    "         'Jan04a','Jan04b','Jan05a','Jan05b','Jan06a','Jan06b','Jan07a','Jan07b','Jan08a','Jan08b',\n",
    "         'Jan09a','Jan09b','Jan10a','Jan10b','Jan11a','Jan11b','Jan12a','Jan12b','Jan13a','Jan13b',\n",
    "         'Mar04a','Mar04b','Mar05a','Mar05b','Mar06a','Mar06b','Mar07a','Mar07b','Mar08a','Mar08b',\n",
    "         'Mar09a','Mar09b','Mar10a','Mar10b','Mar11a','Mar11b','Mar12a','Mar12b','Mar13a','Mar13b']\n",
    "\n",
    "data_types = ['Eff', 'Grps', 'Whls', 'iso', 'sst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = {'Dec':0, 'Jan':1, 'Feb':2, 'Mar':3}\n",
    "fortnights = {'a':0, 'b':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create unique cell ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_dataset = HabModel_merged.reset_index(level=0).rename(columns={'index':'quad_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create columns for 2-week period, year and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bimonthly_subsets = []\n",
    "for t in time_intervals:\n",
    "    # Build column name list\n",
    "    cols = ['quad_id', 'geometry', 'DistToShor', 'MEAN_depth']\n",
    "    n_fixed = len(cols)\n",
    "    cols += [d+t for d in data_types[:-2]]\n",
    "    cols += [d+t.lower() for d in data_types[-2:]]\n",
    "    \n",
    "    # Remove year time data from column labels\n",
    "    subset = working_dataset[cols].rename(columns=dict(zip(cols[n_fixed:], data_types)))\n",
    "    \n",
    "    # Add time data\n",
    "    month = months[t[:3]]\n",
    "    year = int(t[3:-1])\n",
    "    if not month:\n",
    "        year +=1\n",
    "    fortnight = fortnights[t[-1]]\n",
    "    period = month*2 + fortnight\n",
    "    \n",
    "    subset['period'] = period\n",
    "    subset['year'] = year - 3\n",
    "    subset['time'] = t\n",
    "    \n",
    "    bimonthly_subsets.append(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate subsets into single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_data_long = pd.concat(bimonthly_subsets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the resulting dataset is the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert sum([b.shape[0] for b in bimonthly_subsets]) == working_data_long.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add centroid coordinates to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(working_data_long.geometry.apply(lambda x: dict(zip(['lat','lon'], np.ravel(x.centroid.xy)))).tolist(),\n",
    "            index=working_data_long.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_data_long = working_data_long.join(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract shipping exposure data, primarily by filtering out ineligible transits (Dec-Mar only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transits['start_time'] = pd.to_datetime(transits.TR_START, 'D')\n",
    "transits = transits[transits.start_time.apply(lambda x: (x.month in (12, 1, 2, 3)) and (x.date()<date(2013, 4, 1)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit_times = transits.start_time.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month_lookup = {12: 'Dec', 1: 'Jan', 2: 'Feb', 3: 'Mar'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transits['time'] = transit_times.apply(lambda t: month_lookup[t.month] + str(t.year)[-2:] + ['a','b'][t.day>14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 2003, given the above, and recode year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_data_long['year'] = working_data_long.year - 1\n",
    "working_data_long = working_data_long[working_data_long.year > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_data_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersect polygons with transits and count transit intersections with each polygon as metric of ship traffic. This is a computationally intensive task, so the result will be saved to a shapefile, to be reused in later sessions. Flip `import_data` to `False` to re-generate intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_data = True\n",
    "\n",
    "if not import_data:\n",
    "    working_data_long['traffic'] = None\n",
    "    for t in time_intervals:\n",
    "        lines = [shape(g) for g in transits[transits.time==t].geometry]\n",
    "        working_data_long.loc[working_data_long.time==t, 'traffic'] = working_data_long[\n",
    "            working_data_long.time==t].geometry.apply(lambda x: sum([shape(x).intersects(g) for g in lines]))\n",
    "\n",
    "    gpd.GeoDataFrame(working_data_long).to_file('working_data_long.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if import_data:\n",
    "    working_data_long = gpd.GeoDataFrame.from_file('working_data_long.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vessel traffic maps for an arbitrary period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basemap_sc.proj4string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = plt.gca()\n",
    "basemap_sc.drawstates(linewidth=0.15)\n",
    "basemap_sc.drawcoastlines(linewidth=0.25)\n",
    "basemap_sc.fillcontinents(zorder=-1, color='green', alpha=0.5)\n",
    "\n",
    "gpd.GeoDataFrame(working_data_long[working_data_long.time=='Jan13a']).plot(column='traffic', \n",
    "                                                                           colormap='Reds', linewidth=0, axes=ax)\n",
    "reachesWGS84.plot(axes=ax, colormap='Greys')\n",
    "navchannelWGS84.plot(axes=ax, colormap='Greys')\n",
    "sma.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = working_data_long[~((working_data_long.Eff==0) & (working_data_long.Grps>0)) \n",
    "                            & working_data_long.sst.notnull() & working_data_long.MEAN_depth.notnull()]\n",
    "\n",
    "# Include rows with positive effort\n",
    "not_missing = dataset.Eff>0\n",
    "dataset_surveyed = dataset[not_missing]\n",
    "# Rows with zero effort is treated as missing data, to be predicted\n",
    "missing_data = dataset[~not_missing]\n",
    "centroids = dataset[['lat', 'lon']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract transits to each port by intersecting transits with reaches. Create a buffer around each reach, and identify traffic that enter the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charleston_buffered = reachesWGS84.geometry.buffer(.05).unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charleston_traffic = transits[transits.geometry.intersects(charleston_buffered)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savannah_buffered = navchannelWGS84.geometry.buffer(.05).unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savannah_traffic = transits[transits.geometry.intersects(savannah_buffered)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savannah_traffic.ix[:1600].plot(colormap='Reds')\n",
    "navchannelWGS84.geometry.plot(colormap='Greys', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map shows relative mean survey effort in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = plt.gca()\n",
    "\n",
    "df_geo = dataset.groupby('quad_id')\n",
    "\n",
    "test = dataset[(dataset.year==5) & (dataset.period==5)].copy()\n",
    "test = test.set_index(test.quad_id)\n",
    "\n",
    "test['total_groups'] = df_geo['Grps'].sum()\n",
    "test['mean_effort'] = df_geo['Eff'].mean()\n",
    "\n",
    "basemap_sc.drawstates(linewidth=0.15)\n",
    "basemap_sc.drawcoastlines(linewidth=0.25)\n",
    "basemap_sc.fillcontinents(zorder=-1, color='green', alpha=0.5)\n",
    "gpd.GeoDataFrame(test).plot(column='mean_effort', colormap='Reds', linewidth=0, axes=ax)\n",
    "sma.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conditional autoregressive model of whale occurrence\n",
    "\n",
    "One of the key features of this analysis is the modeling of spatial autocorrelation via Gaussian processes. Since the number of surveys and sightings are limited, we need to borrow strength among the grid cells of the region, recognizing that an animal observed in one cell is likely to also be in neighboring cells within the same period. The Gaussian process models the spatial covariance directly, here estimating a risk surface of whale occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc import Normal, Exponential, Uniform, Uninformative, Bernoulli, Poisson, Gamma\n",
    "from pymc import Lambda, MCMC, invlogit, AdaptiveMetropolis, MCMC, normal_like\n",
    "from pymc import potential, deterministic, stochastic, rnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mesh of area from grid centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latmin, lonmin = centroids.min(0)\n",
    "latmax, lonmax = centroids.max(0)\n",
    "grid = np.meshgrid(np.linspace(lonmin, lonmax), np.linspace(latmin, latmax))\n",
    "geo_mesh = np.vstack((np.ravel(grid[0]), np.ravel(grid[1]))).T*np.pi/180."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract columns of interest from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lat, lon, period, year, sst, iso, traffic, depth) = dataset[\n",
    "    ['lat', 'lon', 'period', 'year', 'sst', 'iso', 'traffic', 'MEAN_depth']].values.T\n",
    "period = period.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grouped dataset to extract summary values for each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_geo = dataset.groupby('quad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns that need to be averaged\n",
    "data_summarized = df_geo[['lat', 'lon', 'sst', 'iso', 'MEAN_depth']].mean()\n",
    "# Columns that need to be summed\n",
    "data_summarized[['traffic', 'Grps', 'Whls', 'Eff']] = df_geo[['traffic', 'Grps', 'Whls', 'Eff']].sum()\n",
    "data_summarized['geometry'] = df_geo['geometry'].first()\n",
    "data_summarized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_summarized.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of effort across cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_summarized.Eff.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unsurveyed polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_surveyed = data_summarized[data_summarized.Eff>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize SST, ISO and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = lambda x: np.array((x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst_norm = normalize(data_surveyed.sst)\n",
    "iso_norm = normalize(data_surveyed.iso)\n",
    "depth_norm = normalize(data_surveyed.MEAN_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traffic = data_surveyed.traffic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups, whales, effort = data_surveyed[['Grps', 'Whls', 'Eff']].values.T\n",
    "year = year.astype(int)\n",
    "groups = groups.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale effort by a factor of 100 to avoid numerical issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effort = effort/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate neighborhood matrix for all polygons, to be used in the first-order conditional autoregressive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gdf = gpd.GeoDataFrame(data_surveyed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighbors = np.array([data_gdf.geometry.touches(v).values for i, v in data_gdf.geometry.iteritems()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will be using a Gaussian radial basis function for the non-parametric regression terms describing the effects of isobath and depth on whale probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaussian_rbf = lambda x, mu, l: np.exp(-np.abs(x - mu)**2 / l**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xrange = np.linspace(-5, 5)\n",
    "plt.plot(xrange, [gaussian_rbf(x, 0, 1) for x in xrange])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric regression sub-model\n",
    "\n",
    "In order to avoid making assumptions about the functional form of the relationship between spatial covariates (water depth and distance to 22-degree isotherm) and the encounter rate with whale groups. For each variable, we specify ten Student-t distributed basis functions that are evenly-spaced over the observed range of the variables. The parameters corresponding to each basis function are then weighted according to the distance from a given point to the center of the function, and added to form a coefficient. The result is a flexible, non-parametric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rbf(name, x, n=10):\n",
    "    \n",
    "    # Number of basis functions for isobath effect\n",
    "    h = np.linspace(x.min(), x.max(), 10)\n",
    "    l = Exponential('l_%s' % name, 1, value=1)\n",
    "    # RBF weights\n",
    "    w = Lambda('w_%s' % name, lambda l=l: np.array([gaussian_rbf(i, h, l) for i in x]))\n",
    "    \n",
    "    σ = Gamma('σ_%s' % name, 0.5, 0.5, value=np.ones(n))\n",
    "    τ = σ**-2\n",
    "\n",
    "    β = Normal('β_%s' % name, 0, τ, value=np.zeros(n))\n",
    "    τ_rbf = Exponential('τ_rbf_%s' % name, 1, value=1)\n",
    "    \n",
    "    @stochastic(name='rbf_%s' % name)\n",
    "    def rbf(value=np.zeros(len(x)), β=β, w=w, τ=τ_rbf):\n",
    "        m = w.dot(β)\n",
    "        return normal_like(value, m, τ)\n",
    "    \n",
    "    @deterministic(name='rbf_eval_%s' % name)\n",
    "    def rbf_eval(β=β, τ=τ_rbf, l=l):\n",
    "        wts = np.array([gaussian_rbf(i, h, l) for i in h])\n",
    "        m = wts.dot(β)\n",
    "        return rnormal(m, τ)\n",
    "    \n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional autoregressive (CAR) model\n",
    "\n",
    "In order to allow for the spatial autocorrelation of whale group observations, we implemented a first-order conditional autoregressive model, which expresses the effect in the current cell $k$ as a Gaussian-distributed random variable, with a mean equal to the weighted average of the direct neighbors (typically 8 cells). \n",
    "\n",
    "![CAR](http://d.pr/i/11AkN+)\n",
    "\n",
    "Here, we equally weight all neighbors, $w_{ki} = 1 \\, \\forall \\, i,k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rbf_car_model():\n",
    "    \n",
    "    intercept = Normal('intercept', 0, 0.001, value=0)\n",
    "    \n",
    "    rbf_iso = rbf('iso', iso_norm)\n",
    "    \n",
    "    rbf_depth = rbf('depth', depth_norm)\n",
    "    \n",
    "    # CAR component\n",
    "    ϕ = Uninformative('ϕ', value=np.zeros(len(groups)))\n",
    "    τ_car = Exponential('τ_car', 1, value=1)\n",
    "    \n",
    "    @potential\n",
    "    def car(ϕ=ϕ, τ=τ_car):\n",
    "        \n",
    "        n = neighbors.sum(0).astype(float)\n",
    "        \n",
    "        return normal_like(ϕ, ϕ.dot(neighbors)/n, n*τ)\n",
    "\n",
    "    # Occurrence rate\n",
    "    @deterministic\n",
    "    def θ(β0=intercept, β1=rbf_depth['rbf'], β2=rbf_iso['rbf'], phi=ϕ):\n",
    "        return np.exp(β0 + β1 + β2 + phi) \n",
    "\n",
    "    λ = Lambda('λ', lambda θ=θ: θ * effort)\n",
    "        \n",
    "    # Likelihood\n",
    "    group_obs = Poisson('group_obs', λ, value=groups, observed=True)\n",
    "    \n",
    "    risk = Lambda('risk', lambda theta=θ: theta*traffic)\n",
    "               \n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and run MCMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = MCMC(rbf_car_model())\n",
    "\n",
    "# Use adaptive Metropolis to improve mixing for some variables\n",
    "M.use_step_method(AdaptiveMetropolis, M.__dict__['rbf_iso']['σ'])\n",
    "M.use_step_method(AdaptiveMetropolis, M.__dict__['rbf_depth']['σ'])\n",
    "M.use_step_method(AdaptiveMetropolis, M.ϕ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.sample(200000, 190000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output\n",
    "\n",
    "The following graphical summaries show the posterior distribution of key parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis function variances for isobath effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import Matplot\n",
    "\n",
    "Matplot.summary_plot(M.__dict__['rbf_iso']['σ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.plot(M.__dict__['rbf_iso']['τ_rbf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.plot(M.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-parametric depth effect (recall that depth is expressed in negative values in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.summary_plot(M.__dict__['rbf_depth']['rbf_eval'], \n",
    "                     custom_labels=np.linspace(iso_norm.min(), iso_norm.max(), 10).round(1).astype(str),\n",
    "                    xlab='Effect size', main='Normalized depth effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.summary_plot(M.__dict__['rbf_depth']['β'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-parametric isotherm effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.summary_plot(M.__dict__['rbf_iso']['rbf_eval'],\n",
    "                     custom_labels=np.linspace(iso_norm.min(), iso_norm.max(), 10).round(1).astype(str),\n",
    "                    xlab='Effect size', main='Normalized isotherm effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.summary_plot(M.__dict__['rbf_iso']['β'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_surveyed['p_pred'] = M.θ.stats()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_series(series, data=data_surveyed, cmap='Reds'):\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(data)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = plt.gca()\n",
    "    basemap_sc.drawstates(linewidth=0.15)\n",
    "    basemap_sc.drawcoastlines(linewidth=0.25)\n",
    "    basemap_sc.fillcontinents(zorder=-1, color='green', alpha=0.5)\n",
    "    gdf.plot(column=series, colormap=cmap, linewidth=0, axes=ax)\n",
    "    navchannelWGS84.plot(column='Color', axes=ax)\n",
    "    reachesWGS84.plot(column='reachCode', axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected encounter rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_series('p_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate of risk based on past traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_surveyed['risk'] = M.risk.stats()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_series('risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial autocorrelation effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_surveyed['phi'] = M.ϕ.stats()['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_series('phi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic on the logarithmic scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_surveyed['log_traffic'] = np.log(data_surveyed.traffic + 0.01)\n",
    "\n",
    "plot_series('log_traffic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total past traffic relative ot reaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = plt.gca()\n",
    "basemap_sc.drawstates(linewidth=0.15)\n",
    "basemap_sc.drawcoastlines(linewidth=0.25)\n",
    "basemap_sc.fillcontinents(zorder=-1, color='green', alpha=0.5)\n",
    "\n",
    "gpd.GeoDataFrame(data_surveyed).plot(column='traffic', colormap='Reds', linewidth=0, axes=ax)\n",
    "reachesWGS84.plot(axes=ax, colormap='Greys')\n",
    "navchannelWGS84.plot(axes=ax, colormap='Greys')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
